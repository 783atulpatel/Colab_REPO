{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/783atulpatel/Colab_REPO/blob/main/how_working_install_Hadoop_with_Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1: HDFS (Hadoop Distributed File System)**\n",
        "- Why First?:\n",
        "  It is the foundation for Hadoop and the primary storage system.\n",
        "- Demo Focus:\n",
        "-\tSetup HDFS.\n",
        "- Basic HDFS commands (upload, download, list files, replication, etc.).\n",
        "- Use hdfs Python libraries to interact with HDFS.\n",
        "\n",
        "*   Real-Time Example: Store log files or CSV data into HDFS.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ftBQI1mEkKip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2: Installing Hadoop and setting up in Colab**"
      ],
      "metadata": {
        "id": "PwcLKHl-k_84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # prompt: setup and install hadoop in notebook\n",
        "\n",
        "# Install Java Development Kit (JDK)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Hadoop\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "\n",
        "# Extract Hadoop\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "\n"
      ],
      "metadata": {
        "id": "Qo3XHMBl75B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hadoop environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.environ[\"HADOOP_HOME\"] + \"/bin\" + \":\" + os.environ[\"HADOOP_HOME\"] + \"/sbin\"\n",
        "\n",
        "# Configure Hadoop (replace with your desired configuration)\n",
        "!echo \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> ~/.bashrc\n",
        "!echo \"export HADOOP_HOME=/content/hadoop-3.3.6\" >> ~/.bashrc\n",
        "!echo \"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\" >> ~/.bashrc\n",
        "\n",
        "# Source the bashrc file to apply the changes\n",
        "!source ~/.bashrc\n",
        "\n",
        "# Verify Hadoop installation\n",
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb--yj1tYiEV",
        "outputId": "3ec9167b-ca25-404e-8dd4-7df3bf2183af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8dKh0XIar9k",
        "outputId": "7d9b423f-0464-4e73-f40e-990972143b0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcfSc7X6auvU",
        "outputId": "8633ea0c-072a-40ea-bf72-1eb378dea764"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 09:34:20,925 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = eab42aca2b21/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.3.6/etc/hadoop:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-kms-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okio-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn:/content/hadoop-3.3.6/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/java-util-1.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-commons-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-tree-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/json-io-2.5.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
            "STARTUP_MSG:   java = 1.8.0_432\n",
            "************************************************************/\n",
            "2024-12-20 09:34:21,020 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-12-20 09:34:21,231 INFO namenode.NameNode: createNameNode [-format]\n",
            "2024-12-20 09:34:21,877 INFO namenode.NameNode: Formatting using clusterid: CID-558c1922-88f1-407a-95e1-da0b265fb0eb\n",
            "2024-12-20 09:34:21,950 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-12-20 09:34:21,991 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-12-20 09:34:21,994 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-12-20 09:34:21,995 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-12-20 09:34:22,003 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2024-12-20 09:34:22,003 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2024-12-20 09:34:22,003 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2024-12-20 09:34:22,003 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2024-12-20 09:34:22,003 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-12-20 09:34:22,070 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-12-20 09:34:22,242 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2024-12-20 09:34:22,242 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-12-20 09:34:22,246 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-12-20 09:34:22,247 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Dec 20 09:34:22\n",
            "2024-12-20 09:34:22,249 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-12-20 09:34:22,249 INFO util.GSet: VM type       = 64-bit\n",
            "2024-12-20 09:34:22,251 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-12-20 09:34:22,251 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-12-20 09:34:22,315 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-12-20 09:34:22,315 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-12-20 09:34:22,336 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2024-12-20 09:34:22,336 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-12-20 09:34:22,336 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-12-20 09:34:22,337 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-12-20 09:34:22,338 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-12-20 09:34:22,400 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-12-20 09:34:22,401 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-12-20 09:34:22,401 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-12-20 09:34:22,401 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-12-20 09:34:22,426 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-12-20 09:34:22,426 INFO util.GSet: VM type       = 64-bit\n",
            "2024-12-20 09:34:22,426 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-12-20 09:34:22,426 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-12-20 09:34:22,534 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2024-12-20 09:34:22,534 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-12-20 09:34:22,534 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-12-20 09:34:22,535 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-12-20 09:34:22,545 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-12-20 09:34:22,547 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-12-20 09:34:22,554 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-12-20 09:34:22,554 INFO util.GSet: VM type       = 64-bit\n",
            "2024-12-20 09:34:22,554 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-12-20 09:34:22,554 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-12-20 09:34:22,574 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-12-20 09:34:22,575 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-12-20 09:34:22,575 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-12-20 09:34:22,582 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-12-20 09:34:22,582 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-12-20 09:34:22,587 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-12-20 09:34:22,587 INFO util.GSet: VM type       = 64-bit\n",
            "2024-12-20 09:34:22,587 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-12-20 09:34:22,587 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "Re-format filesystem in Storage Directory root= /tmp/hadoop-root/dfs/name; location= null ? (Y or N) Y\n",
            "2024-12-20 09:34:29,772 INFO namenode.FSImage: Allocated new BlockPoolId: BP-393092624-172.28.0.12-1734687269759\n",
            "2024-12-20 09:34:29,772 INFO common.Storage: Will remove files: [/tmp/hadoop-root/dfs/name/current/VERSION, /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000, /tmp/hadoop-root/dfs/name/current/seen_txid, /tmp/hadoop-root/dfs/name/current/fsimage_0000000000000000000.md5]\n",
            "2024-12-20 09:34:29,793 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-12-20 09:34:29,953 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-12-20 09:34:30,088 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\n",
            "2024-12-20 09:34:30,099 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-12-20 09:34:30,141 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-12-20 09:34:30,142 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-12-20 09:34:30,147 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-12-20 09:34:30,148 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at eab42aca2b21/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!hdfs namenode -format\n",
        "!start-dfs.sh\n",
        "!start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q9fNMemYmDt",
        "outputId": "2901702f-bdda-401f-d9f7-65ce586236b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [eab42aca2b21]\n",
            "ERROR: Attempting to operate on hdfs namenode as root\n",
            "ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.\n",
            "Starting datanodes\n",
            "ERROR: Attempting to operate on hdfs datanode as root\n",
            "ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.\n",
            "Starting secondary namenodes [eab42aca2b21]\n",
            "ERROR: Attempting to operate on hdfs secondarynamenode as root\n",
            "ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.\n",
            "Starting resourcemanager\n",
            "ERROR: Attempting to operate on yarn resourcemanager as root\n",
            "ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.\n",
            "Starting nodemanagers\n",
            "ERROR: Attempting to operate on yarn nodemanager as root\n",
            "ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if Hadoop services are running\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pkAn3TNae-t",
        "outputId": "13c94b65-b95f-44b6-805a-3359b7bcf06a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11420 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3:Practicing Hadoop From Basic To Advance**"
      ],
      "metadata": {
        "id": "gXAVl11wk0_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list down the  current directory\n",
        "!hdfs dfs -ls /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRsLMAGqa-aM",
        "outputId": "fd33c2d6-bd72-47d1-b9e2-630c37ee3d5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29 items\n",
            "-rwxr-xr-x   1 root root          0 2024-12-20 08:53 /.dockerenv\n",
            "-rw-r--r--   1 root root      17294 2023-11-10 04:55 /NGC-DL-CONTAINER-LICENSE\n",
            "drwxr-xr-x   - root root       4096 2024-12-20 09:02 /bin\n",
            "drwxr-xr-x   - root root       4096 2022-04-18 10:28 /boot\n",
            "drwxr-xr-x   - root root       4096 2024-12-20 09:12 /content\n",
            "-rw-r--r--   1 root root       4332 2023-11-10 04:56 /cuda-keyring_1.0-1_all.deb\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:32 /datalab\n",
            "drwxr-xr-x   - root root        360 2024-12-20 08:53 /dev\n",
            "drwxr-xr-x   - root root       4096 2024-12-20 09:02 /etc\n",
            "drwxr-xr-x   - root root       4096 2022-04-18 10:28 /home\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:19 /lib\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:18 /lib32\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:18 /lib64\n",
            "drwxr-xr-x   - root root       4096 2023-10-04 02:08 /libx32\n",
            "drwxr-xr-x   - root root       4096 2023-10-04 02:08 /media\n",
            "drwxr-xr-x   - root root       4096 2023-10-04 02:08 /mnt\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:47 /opt\n",
            "dr-xr-xr-x   - root root          0 2024-12-20 08:53 /proc\n",
            "drwxrwxr-x   - root root       4096 2024-12-18 14:21 /python-apt\n",
            "-r-xr-xr-x   1 root root     346012 2000-01-01 08:00 /python-apt.tar.xz\n",
            "drwx------   - root root       4096 2024-12-20 08:53 /root\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:15 /run\n",
            "drwxr-xr-x   - root root       4096 2024-12-20 08:53 /sbin\n",
            "drwxr-xr-x   - root root       4096 2023-10-04 02:08 /srv\n",
            "dr-xr-xr-x   - root root          0 2024-12-20 08:53 /sys\n",
            "drwxrwxrwt   - root root       4096 2024-12-20 09:34 /tmp\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:32 /tools\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:33 /usr\n",
            "drwxr-xr-x   - root root       4096 2024-12-18 14:32 /var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the Hadoop NameNode's port\n",
        "# !grep dfs.namenode.http-address /content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\n",
        "\n",
        "# # Install ngrok (if not already installed)\n",
        "# !wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# # Expose the NameNode port to the internet using ngrok\n",
        "# # Replace '50070' with your actual NameNode port if different\n",
        "# # This assumes port 50070 from hdfs-site.xml\n",
        "# get_ipython().system_raw('./ngrok http 50070 &')\n",
        "\n",
        "# # Get the public URL from ngrok\n",
        "# !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "0Jp313VIcZ9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: colab ip find cmd\n",
        "\n",
        "import os\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Check if Hadoop services are running\n",
        "!jps\n",
        "\n",
        "# Find the IP address of the Colab instance\n",
        "!curl -s ifconfig.me"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79rOxOh4xOYI",
        "outputId": "4d7153ff-b986-48ad-a94e-f0101c14a497"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36567 Jps\n",
            "34.75.232.59"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory in HDFS\n",
        "!hdfs dfs -mkdir /content/new_folder"
      ],
      "metadata": {
        "id": "_FR8_bBqbFws"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a file to HDFS\n",
        "!hdfs dfs -put sample.txt /content/new_folder"
      ],
      "metadata": {
        "id": "fMTzcfm1dPb8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List files in a directory:\n",
        "!hdfs dfs -ls /content/new_folder/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2BwhDORek5T",
        "outputId": "72877853-e55d-43db-ae0d-d7b2479ae652"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root         29 2024-12-20 09:48 /content/new_folder/sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the content of file\n",
        "!hdfs dfs -cat /content/new_folder/sample.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtiU_GIbevPD",
        "outputId": "911f8175-3324-4370-c781-dbf213404e98"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello i am learning hadoop \r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and write to a new file in the new_folder\n",
        "with open('/content/new_folder/new_sample.txt', 'w') as f:\n",
        "    f.write(\"This is a new sample text in the newly created file.\")\n"
      ],
      "metadata": {
        "id": "ME2Zce5Ui9Wh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verifying rthe file is created or not\n",
        "!hdfs dfs -ls /content/new_folder/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVm2FVVwjCB7",
        "outputId": "9cf3b650-16aa-4e73-9a8c-86c4091a1450"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root         52 2024-12-20 10:10 /content/new_folder/new_sample.txt\n",
            "-rw-r--r--   1 root root         55 2024-12-20 10:06 /content/new_folder/sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_SfRoLN8kJFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the content of updated file\n",
        "!hdfs dfs -cat /content/new_folder/new_sample.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS5qNjnOjOB5",
        "outputId": "8ef448d0-85c3-457d-f051-ac341e67b4a5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a new sample text in the newly created file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the file to local\n",
        "!hdfs dfs -put /content/new_folder/new_sample.txt /content/"
      ],
      "metadata": {
        "id": "n5VVqFBNjYWY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the old file from the newfolder\n",
        "!hdfs dfs -rm /content/new_folder/sample.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uUIKSZdlPR5",
        "outputId": "0c0151c8-a7e7-4de9-8332-c88490e3d135"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 10:21:18,213 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/new_folder/sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. MapReduce (Basic Job Execution)**\n",
        "###What We will Learn in this :\n",
        "####how to run a MapReduce job, including creating and executing jobs on HDFS."
      ],
      "metadata": {
        "id": "jrqzvxyJmGLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so initially will create an input.txt file\n",
        "with open('/content/new_folder/input.txt', 'w') as f:\n",
        "    f.write(\"Hello HadoopHadoop MapReduce is great for big dataBig data can be processed using Hadoop MapReduce is a programming model\")\n"
      ],
      "metadata": {
        "id": "zDXa0NC5lfNo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /content/new_folder/input.txt /content/new_folder/output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avcWK6Q9nFlo",
        "outputId": "9727f20d-3bb1-45f6-e100-d6516a3e3a8b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 10:43:47,249 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-12-20 10:43:47,383 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-12-20 10:43:47,383 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-12-20 10:43:47,675 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-12-20 10:43:47,716 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-12-20 10:43:48,005 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1541016117_0001\n",
            "2024-12-20 10:43:48,005 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-12-20 10:43:48,207 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-12-20 10:43:48,208 INFO mapreduce.Job: Running job: job_local1541016117_0001\n",
            "2024-12-20 10:43:48,215 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-12-20 10:43:48,227 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-12-20 10:43:48,229 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-12-20 10:43:48,229 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-12-20 10:43:48,230 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-12-20 10:43:48,285 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-12-20 10:43:48,286 INFO mapred.LocalJobRunner: Starting task: attempt_local1541016117_0001_m_000000_0\n",
            "2024-12-20 10:43:48,322 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-12-20 10:43:48,322 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-12-20 10:43:48,322 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-12-20 10:43:48,362 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-12-20 10:43:48,377 INFO mapred.MapTask: Processing split: file:/content/new_folder/input.txt:0+121\n",
            "2024-12-20 10:43:48,592 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-12-20 10:43:48,592 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-12-20 10:43:48,592 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-12-20 10:43:48,592 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-12-20 10:43:48,592 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-12-20 10:43:48,599 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-12-20 10:43:48,615 INFO mapred.LocalJobRunner: \n",
            "2024-12-20 10:43:48,615 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-12-20 10:43:48,615 INFO mapred.MapTask: Spilling map output\n",
            "2024-12-20 10:43:48,616 INFO mapred.MapTask: bufstart = 0; bufend = 198; bufvoid = 104857600\n",
            "2024-12-20 10:43:48,616 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214324(104857296); length = 73/6553600\n",
            "2024-12-20 10:43:48,647 INFO mapred.MapTask: Finished spill 0\n",
            "2024-12-20 10:43:48,680 INFO mapred.Task: Task:attempt_local1541016117_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-12-20 10:43:48,683 INFO mapred.LocalJobRunner: map\n",
            "2024-12-20 10:43:48,683 INFO mapred.Task: Task 'attempt_local1541016117_0001_m_000000_0' done.\n",
            "2024-12-20 10:43:48,696 INFO mapred.Task: Final Counters for attempt_local1541016117_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281622\n",
            "\t\tFILE: Number of bytes written=919889\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=19\n",
            "\t\tMap output bytes=198\n",
            "\t\tMap output materialized bytes=217\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=19\n",
            "\t\tCombine output records=17\n",
            "\t\tSpilled Records=17\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=121\n",
            "2024-12-20 10:43:48,697 INFO mapred.LocalJobRunner: Finishing task: attempt_local1541016117_0001_m_000000_0\n",
            "2024-12-20 10:43:48,698 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-12-20 10:43:48,703 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-12-20 10:43:48,704 INFO mapred.LocalJobRunner: Starting task: attempt_local1541016117_0001_r_000000_0\n",
            "2024-12-20 10:43:48,719 INFO output.PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
            "2024-12-20 10:43:48,719 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-12-20 10:43:48,719 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-12-20 10:43:48,719 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-12-20 10:43:48,727 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@59fd4aee\n",
            "2024-12-20 10:43:48,756 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-12-20 10:43:48,788 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-12-20 10:43:48,790 INFO reduce.EventFetcher: attempt_local1541016117_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-12-20 10:43:48,836 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1541016117_0001_m_000000_0 decomp: 213 len: 217 to MEMORY\n",
            "2024-12-20 10:43:48,840 INFO reduce.InMemoryMapOutput: Read 213 bytes from map-output for attempt_local1541016117_0001_m_000000_0\n",
            "2024-12-20 10:43:48,842 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 213, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->213\n",
            "2024-12-20 10:43:48,854 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-12-20 10:43:48,856 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-12-20 10:43:48,856 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-12-20 10:43:48,864 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-12-20 10:43:48,864 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 204 bytes\n",
            "2024-12-20 10:43:48,866 INFO reduce.MergeManagerImpl: Merged 1 segments, 213 bytes to disk to satisfy reduce memory limit\n",
            "2024-12-20 10:43:48,866 INFO reduce.MergeManagerImpl: Merging 1 files, 217 bytes from disk\n",
            "2024-12-20 10:43:48,867 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-12-20 10:43:48,867 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-12-20 10:43:48,868 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 204 bytes\n",
            "2024-12-20 10:43:48,869 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-12-20 10:43:48,872 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-12-20 10:43:48,877 INFO mapred.Task: Task:attempt_local1541016117_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-12-20 10:43:48,881 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-12-20 10:43:48,882 INFO mapred.Task: Task attempt_local1541016117_0001_r_000000_0 is allowed to commit now\n",
            "2024-12-20 10:43:48,884 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1541016117_0001_r_000000_0' to file:/content/new_folder/output\n",
            "2024-12-20 10:43:48,885 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-12-20 10:43:48,886 INFO mapred.Task: Task 'attempt_local1541016117_0001_r_000000_0' done.\n",
            "2024-12-20 10:43:48,887 INFO mapred.Task: Final Counters for attempt_local1541016117_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=282088\n",
            "\t\tFILE: Number of bytes written=920261\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=17\n",
            "\t\tReduce shuffle bytes=217\n",
            "\t\tReduce input records=17\n",
            "\t\tReduce output records=17\n",
            "\t\tSpilled Records=17\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=155\n",
            "2024-12-20 10:43:48,888 INFO mapred.LocalJobRunner: Finishing task: attempt_local1541016117_0001_r_000000_0\n",
            "2024-12-20 10:43:48,888 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-12-20 10:43:49,215 INFO mapreduce.Job: Job job_local1541016117_0001 running in uber mode : false\n",
            "2024-12-20 10:43:49,216 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-12-20 10:43:49,217 INFO mapreduce.Job: Job job_local1541016117_0001 completed successfully\n",
            "2024-12-20 10:43:49,225 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=563710\n",
            "\t\tFILE: Number of bytes written=1840150\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=19\n",
            "\t\tMap output bytes=198\n",
            "\t\tMap output materialized bytes=217\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=19\n",
            "\t\tCombine output records=17\n",
            "\t\tReduce input groups=17\n",
            "\t\tReduce shuffle bytes=217\n",
            "\t\tReduce input records=17\n",
            "\t\tReduce output records=17\n",
            "\t\tSpilled Records=34\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=121\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the result of MapReduce WordCount Program\n",
        "!hdfs dfs -cat /content/new_folder/output/part-r-00000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJjSyavynvSh",
        "outputId": "e9c46a21-5d50-4ebc-e4a8-be6db50fa3ea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop\t1\n",
            "HadoopHadoop\t1\n",
            "Hello\t1\n",
            "MapReduce\t2\n",
            "a\t1\n",
            "be\t1\n",
            "big\t1\n",
            "can\t1\n",
            "data\t1\n",
            "dataBig\t1\n",
            "for\t1\n",
            "great\t1\n",
            "is\t2\n",
            "model\t1\n",
            "processed\t1\n",
            "programming\t1\n",
            "using\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: set  ip 0.0.0.0 in .xml file in hadoop etc folder use echo cmd\n",
        "\n",
        "# Find the core-site.xml file\n",
        "!find /content/hadoop-3.3.6/etc/hadoop/ -name core-site.xml\n",
        "\n",
        "# Use sed to replace the existing value with 0.0.0.0\n",
        "# Make a backup of the original file first\n",
        "!cp /content/hadoop-3.3.6/etc/hadoop/core-site.xml /content/hadoop-3.3.6/etc/hadoop/core-site.xml.bak\n",
        "\n",
        "# Replace the IP address using sed\n",
        "!sed -i 's/<name>fs.defaultFS<\\/name>\\s*<value>.*<\\/value>/<name>fs.defaultFS<\\/name>\\n\\t<value>hdfs://0.0.0.0:9000<\\/value>/' /content/hadoop-3.3.6/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "id": "QSRvRQhC0Wnd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. YARN Resource Management**\n",
        "###How to manage resources using YARN (Yet Another Resource Negotiator).\n",
        "\n",
        "###Check YARN Resource Manager UI (You can access the UI through\n",
        "\n",
        "- 34.75.232.59:8088 (after starting YARN daemons):"
      ],
      "metadata": {
        "id": "zUTcdZbgr-Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yarn resourcemanager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjhNK3vMrWbo",
        "outputId": "11a7c1f4-9417-4989-c5ad-abdab1f3e609"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 11:25:25,697 INFO resourcemanager.ResourceManager: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting ResourceManager\n",
            "STARTUP_MSG:   host = eab42aca2b21/172.28.0.12\n",
            "STARTUP_MSG:   args = []\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.3.6/etc/hadoop:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-kms-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okio-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn:/content/hadoop-3.3.6/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/java-util-1.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-commons-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-tree-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/json-io-2.5.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-documentstore-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/commons-digester-1.8.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/rxnetty-0.4.20.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/rxjava-string-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/azure-cosmosdb-2.4.5.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/azure-cosmosdb-gateway-2.4.5.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/azure-cosmosdb-commons-2.4.5.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/azure-cosmosdb-direct-2.4.5.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/commons-collections4-4.2.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/java-uuid-generator-3.1.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/commons-validator-1.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/commons-csv-1.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/rxjava-extras-0.8.0.17.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.4.8.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/hbase-common-1.4.8.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/hbase-client-1.4.8.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/rxjava-1.3.8.jar:/content/hadoop-3.3.6/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.4.8.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
            "STARTUP_MSG:   java = 1.8.0_432\n",
            "************************************************************/\n",
            "2024-12-20 11:25:25,718 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-12-20 11:25:26,373 INFO resourcemanager.RMNMInfo: Registered RMInfo MBean\n",
            "2024-12-20 11:25:26,389 INFO conf.Configuration: found resource core-site.xml at file:/content/hadoop-3.3.6/etc/hadoop/core-site.xml\n",
            "2024-12-20 11:25:26,469 INFO conf.Configuration: resource-types.xml not found\n",
            "2024-12-20 11:25:26,469 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2024-12-20 11:25:26,535 INFO conf.Configuration: found resource yarn-site.xml at file:/content/hadoop-3.3.6/etc/hadoop/yarn-site.xml\n",
            "2024-12-20 11:25:26,543 INFO metrics.GenericEventTypeMetrics: Registering GenericEventTypeMetrics\n",
            "2024-12-20 11:25:26,546 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\n",
            "2024-12-20 11:25:26,598 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\n",
            "2024-12-20 11:25:26,603 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\n",
            "2024-12-20 11:25:26,609 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\n",
            "2024-12-20 11:25:26,698 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\n",
            "2024-12-20 11:25:26,702 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\n",
            "2024-12-20 11:25:26,702 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\n",
            "2024-12-20 11:25:26,764 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher\n",
            "2024-12-20 11:25:26,765 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\n",
            "2024-12-20 11:25:26,767 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\n",
            "2024-12-20 11:25:26,768 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\n",
            "2024-12-20 11:25:26,893 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-12-20 11:25:26,911 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-12-20 11:25:26,911 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\n",
            "2024-12-20 11:25:26,947 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\n",
            "2024-12-20 11:25:26,950 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\n",
            "2024-12-20 11:25:26,958 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\n",
            "2024-12-20 11:25:27,837 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\n",
            "2024-12-20 11:25:27,837 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\n",
            "2024-12-20 11:25:27,846 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\n",
            "2024-12-20 11:25:27,850 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\n",
            "2024-12-20 11:25:27,859 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/content/hadoop-3.3.6/etc/hadoop/capacity-scheduler.xml\n",
            "2024-12-20 11:25:27,869 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\n",
            "2024-12-20 11:25:27,869 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\n",
            "2024-12-20 11:25:27,970 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,\n",
            ", reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\n",
            "2024-12-20 11:25:27,970 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\n",
            "2024-12-20 11:25:27,998 INFO capacity.LeafQueue: Initializing root.default\n",
            "capacity = 1.0 [= (float) configuredCapacity / 100 ]\n",
            "absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\n",
            "maxCapacity = 1.0 [= configuredMaxCapacity ]\n",
            "absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\n",
            "effectiveMinResource=<memory:0, vCores:0>\n",
            " , effectiveMaxResource=<memory:0, vCores:0>\n",
            "userLimit = 100 [= configuredUserLimit ]\n",
            "userLimitFactor = 1.0 [= configuredUserLimitFactor ]\n",
            "maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\n",
            "maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\n",
            "maxParallelApps = 2147483647\n",
            "usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\n",
            "absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\n",
            "maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\n",
            "minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\n",
            "maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\n",
            "numContainers = 0 [= currentNumContainers ]\n",
            "state = RUNNING [= configuredState ]\n",
            "acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\n",
            "nodeLocalityDelay = 40\n",
            "rackLocalityAdditionalDelay = -1\n",
            "labels=*,\n",
            "reservationsContinueLooking = true\n",
            "preemptionDisabled = true\n",
            "defaultAppPriorityPerQueue = 0\n",
            "priority = 0\n",
            "maxLifetime = -1 seconds\n",
            "defaultLifetime = -1 seconds\n",
            "2024-12-20 11:25:27,998 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root.default\n",
            "2024-12-20 11:25:27,999 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root\n",
            "2024-12-20 11:25:28,003 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\n",
            "2024-12-20 11:25:28,004 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\n",
            "2024-12-20 11:25:28,004 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\n",
            "2024-12-20 11:25:28,006 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \n",
            "2024-12-20 11:25:28,006 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false, assignMultipleEnabled=true, maxAssignPerHeartbeat=100, offswitchPerHeartbeatLimit=1\n",
            "2024-12-20 11:25:28,010 INFO conf.Configuration: dynamic-resources.xml not found\n",
            "2024-12-20 11:25:28,013 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\n",
            "2024-12-20 11:25:28,014 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\n",
            "2024-12-20 11:25:28,016 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \n",
            "2024-12-20 11:25:28,024 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\n",
            "2024-12-20 11:25:28,195 INFO util.log: Logging initialized @3307ms to org.eclipse.jetty.util.log.Slf4jLog\n",
            "2024-12-20 11:25:28,310 WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /root/hadoop-http-auth-signature-secret\n",
            "2024-12-20 11:25:28,317 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\n",
            "2024-12-20 11:25:28,326 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
            "2024-12-20 11:25:28,333 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\n",
            "2024-12-20 11:25:28,333 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\n",
            "2024-12-20 11:25:28,333 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\n",
            "2024-12-20 11:25:28,334 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\n",
            "2024-12-20 11:25:28,334 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
            "2024-12-20 11:25:28,334 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
            "2024-12-20 11:25:28,951 INFO webapp.WebApps: Registered webapp guice modules\n",
            "2024-12-20 11:25:28,960 INFO http.HttpServer2: Jetty bound to port 8088\n",
            "2024-12-20 11:25:28,962 INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_432-8u432-ga~us1-0ubuntu2~22.04-ga\n",
            "2024-12-20 11:25:29,014 INFO server.session: DefaultSessionIdManager workerName=node0\n",
            "2024-12-20 11:25:29,014 INFO server.session: No SessionScavenger set, using defaults\n",
            "2024-12-20 11:25:29,016 INFO server.session: node0 Scavenging every 600000ms\n",
            "2024-12-20 11:25:29,042 WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /root/hadoop-http-auth-signature-secret\n",
            "2024-12-20 11:25:29,059 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
            "2024-12-20 11:25:29,063 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n",
            "2024-12-20 11:25:29,064 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
            "2024-12-20 11:25:29,098 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3ecd267f{logs,/logs,file:///content/hadoop-3.3.6/logs/,AVAILABLE}\n",
            "2024-12-20 11:25:29,099 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c9bfddc{static,/static,jar:file:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar!/webapps/static,AVAILABLE}\n",
            "Dec 20, 2024 11:25:29 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
            "INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\n",
            "Dec 20, 2024 11:25:29 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
            "INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\n",
            "Dec 20, 2024 11:25:29 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
            "INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\n",
            "Dec 20, 2024 11:25:29 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\n",
            "INFO: Initiating Jersey application, version 'Jersey: 1.19.4 05/24/2017 03:20 PM'\n",
            "Dec 20, 2024 11:25:29 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
            "INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
            "Dec 20, 2024 11:25:30 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
            "INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
            "Dec 20, 2024 11:25:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
            "INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
            "2024-12-20 11:25:31,231 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@70972170{cluster,/,file:///tmp/jetty-0_0_0_0-8088-hadoop-yarn-common-3_3_6_jar-_-any-418992671632029496/webapp/,AVAILABLE}{jar:file:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar!/webapps/cluster}\n",
            "2024-12-20 11:25:31,243 INFO server.AbstractConnector: Started ServerConnector@3d3ba765{HTTP/1.1, (http/1.1)}{0.0.0.0:8088}\n",
            "2024-12-20 11:25:31,243 INFO server.Server: Started @6355ms\n",
            "2024-12-20 11:25:31,243 INFO webapp.WebApps: Web app cluster started at 8088\n",
            "2024-12-20 11:25:31,315 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\n",
            "2024-12-20 11:25:31,331 INFO ipc.Server: Listener at 0.0.0.0:8033\n",
            "2024-12-20 11:25:31,332 INFO ipc.Server: Starting Socket Reader #1 for port 8033\n",
            "2024-12-20 11:25:31,577 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\n",
            "2024-12-20 11:25:31,578 INFO ipc.Server: IPC Server Responder: starting\n",
            "2024-12-20 11:25:31,579 INFO ipc.Server: IPC Server listener on 8033: starting\n",
            "2024-12-20 11:25:31,580 INFO resourcemanager.ResourceManager: Transitioning to active state\n",
            "2024-12-20 11:25:31,597 INFO recovery.RMStateStore: Updating AMRMToken\n",
            "2024-12-20 11:25:31,598 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\n",
            "2024-12-20 11:25:31,598 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\n",
            "2024-12-20 11:25:31,598 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
            "2024-12-20 11:25:31,600 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\n",
            "2024-12-20 11:25:31,600 INFO recovery.RMStateStore: Storing RMDTMasterKey.\n",
            "2024-12-20 11:25:31,602 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n",
            "2024-12-20 11:25:31,602 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
            "2024-12-20 11:25:31,602 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\n",
            "2024-12-20 11:25:31,602 INFO recovery.RMStateStore: Storing RMDTMasterKey.\n",
            "2024-12-20 11:25:31,605 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\n",
            "2024-12-20 11:25:31,730 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\n",
            "2024-12-20 11:25:31,838 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\n",
            "2024-12-20 11:25:31,838 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\n",
            "2024-12-20 11:25:31,863 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\n",
            "2024-12-20 11:25:31,864 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\n",
            "2024-12-20 11:25:31,882 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\n",
            "2024-12-20 11:25:31,883 INFO ipc.Server: Listener at 0.0.0.0:8031\n",
            "2024-12-20 11:25:31,887 INFO ipc.Server: Starting Socket Reader #1 for port 8031\n",
            "2024-12-20 11:25:31,890 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\n",
            "2024-12-20 11:25:31,891 INFO ipc.Server: IPC Server Responder: starting\n",
            "2024-12-20 11:25:31,891 INFO ipc.Server: IPC Server listener on 8031: starting\n",
            "2024-12-20 11:25:31,915 INFO util.JvmPauseMonitor: Starting JVM pause monitor\n",
            "2024-12-20 11:25:31,936 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\n",
            "2024-12-20 11:25:31,944 INFO ipc.Server: Listener at 0.0.0.0:8030\n",
            "2024-12-20 11:25:31,945 INFO ipc.Server: Starting Socket Reader #1 for port 8030\n",
            "2024-12-20 11:25:31,957 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\n",
            "2024-12-20 11:25:31,958 INFO ipc.Server: IPC Server Responder: starting\n",
            "2024-12-20 11:25:31,958 INFO ipc.Server: IPC Server listener on 8030: starting\n",
            "2024-12-20 11:25:32,096 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\n",
            "2024-12-20 11:25:32,096 INFO ipc.Server: Listener at 0.0.0.0:8032\n",
            "2024-12-20 11:25:32,097 INFO ipc.Server: Starting Socket Reader #1 for port 8032\n",
            "2024-12-20 11:25:32,102 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\n",
            "2024-12-20 11:25:32,103 INFO ipc.Server: IPC Server Responder: starting\n",
            "2024-12-20 11:25:32,104 INFO ipc.Server: IPC Server listener on 8032: starting\n",
            "2024-12-20 11:25:33,024 INFO webproxy.ProxyCA: Created Certificate for OU=YARN-42722840-3e21-4761-adcd-b439023f5d40\n",
            "2024-12-20 11:25:33,235 INFO recovery.RMStateStore: Storing CA Certificate and Private Key\n",
            "2024-12-20 11:25:33,235 INFO resourcemanager.ResourceManager: Transitioned to active state\n",
            "2024-12-20 11:28:23,853 ERROR resourcemanager.ResourceManager: RECEIVED SIGNAL 2: SIGINT\n",
            "2024-12-20 11:28:23,866 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\n",
            "2024-12-20 11:28:23,874 INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@70972170{cluster,/,null,STOPPED}{jar:file:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar!/webapps/cluster}\n",
            "2024-12-20 11:28:23,879 INFO server.AbstractConnector: Stopped ServerConnector@3d3ba765{HTTP/1.1, (http/1.1)}{0.0.0.0:8088}\n",
            "2024-12-20 11:28:23,879 INFO server.session: node0 Stopped scavenging\n",
            "2024-12-20 11:28:23,880 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@3c9bfddc{static,/static,jar:file:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar!/webapps/static,STOPPED}\n",
            "2024-12-20 11:28:23,881 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@3ecd267f{logs,/logs,file:///content/hadoop-3.3.6/logs/,STOPPED}\n",
            "2024-12-20 11:28:23,886 INFO ipc.Server: Stopping server on 8032\n",
            "2024-12-20 11:28:23,893 INFO ipc.Server: Stopping IPC Server listener on 8032\n",
            "2024-12-20 11:28:23,895 INFO ipc.Server: Stopping IPC Server Responder\n",
            "2024-12-20 11:28:23,898 INFO ipc.Server: Stopping server on 8033\n",
            "2024-12-20 11:28:23,900 INFO ipc.Server: Stopping IPC Server listener on 8033\n",
            "2024-12-20 11:28:23,901 INFO ipc.Server: Stopping IPC Server Responder\n",
            "2024-12-20 11:28:23,902 INFO resourcemanager.ResourceManager: Transitioning to standby state\n",
            "2024-12-20 11:28:23,903 WARN amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.\n",
            "2024-12-20 11:28:23,903 INFO ipc.Server: Stopping server on 8030\n",
            "2024-12-20 11:28:23,912 INFO ipc.Server: Stopping IPC Server listener on 8030\n",
            "2024-12-20 11:28:23,913 INFO ipc.Server: Stopping IPC Server Responder\n",
            "2024-12-20 11:28:23,914 INFO ipc.Server: Stopping server on 8031\n",
            "2024-12-20 11:28:23,920 INFO ipc.Server: Stopping IPC Server listener on 8031\n",
            "2024-12-20 11:28:23,923 INFO ipc.Server: Stopping IPC Server Responder\n",
            "2024-12-20 11:28:23,924 INFO util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted\n",
            "2024-12-20 11:28:23,924 ERROR event.EventDispatcher: Returning, interrupted : java.lang.InterruptedException\n",
            "2024-12-20 11:28:23,924 ERROR resourcemanager.ResourceManager: Returning, interrupted : java.lang.InterruptedException: sleep interrupted\n",
            "2024-12-20 11:28:23,925 INFO activities.ActivitiesManager: org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager thread interrupted\n",
            "2024-12-20 11:28:23,927 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\n",
            "2024-12-20 11:28:23,931 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\n",
            "2024-12-20 11:28:23,931 INFO util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor thread interrupted\n",
            "2024-12-20 11:28:23,931 INFO util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted\n",
            "2024-12-20 11:28:23,931 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\n",
            "2024-12-20 11:28:23,931 INFO util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted\n",
            "2024-12-20 11:28:23,932 INFO util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted\n",
            "2024-12-20 11:28:23,932 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\n",
            "2024-12-20 11:28:23,933 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\n",
            "2024-12-20 11:28:23,933 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\n",
            "2024-12-20 11:28:23,933 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\n",
            "2024-12-20 11:28:23,935 INFO resourcemanager.ResourceManager: Transitioned to standby state\n",
            "2024-12-20 11:28:23,936 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down ResourceManager at eab42aca2b21/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run a YARN-based MapReduce Job:**"
      ],
      "metadata": {
        "id": "cFR-Fv9GtJev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yarn jar /content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /content/new_folder/input.txt /content/new_folder/output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgHSXmXAstrH",
        "outputId": "123d4ab3-6a18-43dd-8d51-4f5240154f38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 10:56:40,147 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-12-20 10:56:40,235 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-12-20 10:56:40,235 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/new_folder/output already exists\n",
            "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1678)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1675)\n",
            "\tat java.security.AccessController.doPrivileged(Native Method)\n",
            "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
            "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
            "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1675)\n",
            "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1696)\n",
            "\tat org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
            "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
            "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:328)\n",
            "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:241)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Monitor jobs:**"
      ],
      "metadata": {
        "id": "FmSnN_dots4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yarn application -list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUBO1a8ntldf",
        "outputId": "4afe0239-3775-4e6f-e93a-aca76bc4d4e1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-20 10:57:24,679 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
            "2024-12-20 10:57:26,430 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:27,437 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:28,438 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:29,439 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:30,440 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:31,441 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:32,442 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:33,442 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:34,443 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:35,444 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:36,460 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:37,461 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:38,462 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:39,463 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:40,464 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:41,464 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:42,465 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:43,466 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:44,467 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:45,468 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
            "2024-12-20 10:57:45,469 INFO retry.RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ApplicationClientProtocolPBClientImpl.getApplications over null after 1 failover attempts. Trying to failover after sleeping for 39330ms. Current retry count: 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. Advanced HDFS Commands**"
      ],
      "metadata": {
        "id": "jyVx6O2wuKJf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8_P4Y5_twQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Project :- Store And Process log Data  using Custom MAP REDUCE code**"
      ],
      "metadata": {
        "id": "4sJsmc6SuP1O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPT8grSlwtGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7. Project :- \tRanking Movies by Their Popularity**"
      ],
      "metadata": {
        "id": "6M-KTs-DwjRV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j56ykatrufl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install sql write magic sql query\n",
        "\n",
        "# Install the ipython-sql library\n",
        "!pip install ipython-sql\n",
        "\n",
        "# Load the sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# Connect to a SQLite database (in-memory for this example)\n",
        "%sql sqlite://"
      ],
      "metadata": {
        "id": "IjQGIs_T_xlq",
        "outputId": "3233c0b8-f6a3-4865-a8fc-c0719eb3f49b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (3.12.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (7.34.0)\n",
            "Requirement already satisfied: sqlalchemy>=2.0 in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (2.0.36)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (1.17.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0->ipython-sql) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-sql)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-sql) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->ipython-sql) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a query to create db and sample table\n",
        "\n",
        "%%sql\n",
        "CREATE TABLE IF NOT EXISTS employees (\n",
        "  id INTEGER PRIMARY KEY,\n",
        "  name TEXT NOT NULL,\n",
        "  salary REAL\n",
        ");\n",
        "\n",
        "INSERT INTO employees (name, salary) VALUES ('Alice', 50000.00);\n",
        "INSERT INTO employees (name, salary) VALUES ('Bob', 60000.00);"
      ],
      "metadata": {
        "id": "a3IO2ves_4Xx",
        "outputId": "125bce66-a8bc-4f9f-9f34-504d1e4df0c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * sqlite://\n",
            "Done.\n",
            "1 rows affected.\n",
            "1 rows affected.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oeuPdZRzAqeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}